{
  "id": "602fb84b-6414-4542-b731-fe8cf02dffc0",
  "name": "Test API Project",
  "type": "data_pipeline",
  "status": "created",
  "owner_id": "default_user",
  "created_at": "2025-11-18T09:54:40.952592",
  "updated_at": "2025-11-18T09:54:40.952616",
  "last_deployed_at": null,
  "codebase": {
    "pipeline.py": "import logging\nfrom datetime import datetime\nfrom typing import List, Dict, Any\nimport json\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass DataPipeline:\n    \"\"\"My Test API ETL Pipeline\"\"\"\n    \n    def __init__(self):\n        self.name = \"My Test API\"\n        self.run_id = None\n    \n    def extract(self) -> List[Dict[str, Any]]:\n        \"\"\"Extract data from source\"\"\"\n        logger.info(f\"[{self.run_id}] Starting extraction...\")\n        \n        # TODO: Implement your data extraction logic\n        # Example: Read from database, API, files, etc.\n        data = [\n            {\"id\": 1, \"value\": \"sample1\"},\n            {\"id\": 2, \"value\": \"sample2\"}\n        ]\n        \n        logger.info(f\"[{self.run_id}] Extracted {len(data)} records\")\n        return data\n    \n    def transform(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n        \"\"\"Transform extracted data\"\"\"\n        logger.info(f\"[{self.run_id}] Starting transformation...\")\n        \n        # TODO: Implement your transformation logic\n        # Example: Clean, enrich, aggregate data\n        transformed = []\n        for record in data:\n            transformed.append({\n                **record,\n                \"processed_at\": datetime.utcnow().isoformat(),\n                \"pipeline\": self.name\n            })\n        \n        logger.info(f\"[{self.run_id}] Transformed {len(transformed)} records\")\n        return transformed\n    \n    def load(self, data: List[Dict[str, Any]]) -> bool:\n        \"\"\"Load transformed data to destination\"\"\"\n        logger.info(f\"[{self.run_id}] Starting load...\")\n        \n        # TODO: Implement your load logic\n        # Example: Write to database, data warehouse, files, etc.\n        with open(f\"output_{self.run_id}.json\", \"w\") as f:\n            json.dump(data, f, indent=2)\n        \n        logger.info(f\"[{self.run_id}] Loaded {len(data)} records\")\n        return True\n    \n    def run(self) -> Dict[str, Any]:\n        \"\"\"Execute the complete ETL pipeline\"\"\"\n        self.run_id = datetime.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n        start_time = datetime.utcnow()\n        \n        try:\n            logger.info(f\"[{self.run_id}] Pipeline started\")\n            \n            # Extract\n            raw_data = self.extract()\n            \n            # Transform\n            transformed_data = self.transform(raw_data)\n            \n            # Load\n            success = self.load(transformed_data)\n            \n            end_time = datetime.utcnow()\n            duration = (end_time - start_time).total_seconds()\n            \n            result = {\n                \"run_id\": self.run_id,\n                \"status\": \"success\" if success else \"failed\",\n                \"records_processed\": len(transformed_data),\n                \"duration_seconds\": duration,\n                \"start_time\": start_time.isoformat(),\n                \"end_time\": end_time.isoformat()\n            }\n            \n            logger.info(f\"[{self.run_id}] Pipeline completed: {result}\")\n            return result\n            \n        except Exception as e:\n            logger.error(f\"[{self.run_id}] Pipeline failed: {e}\", exc_info=True)\n            return {\n                \"run_id\": self.run_id,\n                \"status\": \"error\",\n                \"error\": str(e),\n                \"start_time\": start_time.isoformat()\n            }\n\n\nif __name__ == \"__main__\":\n    pipeline = DataPipeline()\n    result = pipeline.run()\n    print(json.dumps(result, indent=2))\n",
    "scheduler.py": "import schedule\nimport time\nfrom pipeline import DataPipeline\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef run_pipeline():\n    \"\"\"Run the pipeline\"\"\"\n    pipeline = DataPipeline()\n    result = pipeline.run()\n    logger.info(f\"Pipeline run completed: {result['status']}\")\n\n\n# Schedule the pipeline\nschedule.every().day.at(\"02:00\").do(run_pipeline)  # Run daily at 2 AM\n# schedule.every().hour.do(run_pipeline)  # Or run hourly\n# schedule.every(30).minutes.do(run_pipeline)  # Or every 30 minutes\n\nlogger.info(\"Scheduler started. Pipeline will run on schedule.\")\n\n# Keep the scheduler running\nwhile True:\n    schedule.run_pending()\n    time.sleep(60)\n",
    "requirements.txt": "schedule==1.2.0\npandas==2.1.3\nsqlalchemy==2.0.23\npsycopg2-binary==2.9.9\n",
    "README.md": "# My Test API\n\nA test API project created from template\n\n## Features\n\n- ETL pipeline with extract, transform, load stages\n- Scheduled execution\n- Error handling and logging\n- Run tracking and monitoring\n\n## Setup\n\n1. Install dependencies:\n```bash\npip install -r requirements.txt\n```\n\n2. Configure your data sources and destinations in `pipeline.py`\n\n3. Run the pipeline manually:\n```bash\npython pipeline.py\n```\n\n4. Run with scheduler:\n```bash\npython scheduler.py\n```\n\n## Pipeline Stages\n\n### Extract\n- Read data from source systems\n- Support for databases, APIs, files\n\n### Transform\n- Clean and validate data\n- Apply business logic\n- Enrich with additional data\n\n### Load\n- Write to destination systems\n- Support for databases, data warehouses, files\n\n## Scheduling\n\nEdit `scheduler.py` to configure run frequency:\n- Daily at specific time\n- Hourly\n- Every N minutes\n\n## Monitoring\n\nPipeline runs are logged with:\n- Run ID\n- Status (success/failed/error)\n- Records processed\n- Duration\n- Timestamps\n"
  },
  "dependencies": [],
  "modifications": [],
  "deployments": [],
  "environment_vars": {},
  "deployment_config": {
    "platform": "render",
    "environment": "production",
    "auto_deploy": false,
    "health_check_enabled": true,
    "monitoring_enabled": false
  },
  "test_coverage": 0.0,
  "security_score": 0.0,
  "performance_score": 0.0,
  "description": "ETL data pipeline with scheduling, monitoring, and error handling",
  "repository_url": null
}