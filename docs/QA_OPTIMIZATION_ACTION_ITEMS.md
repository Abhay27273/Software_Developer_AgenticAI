# QA Agent Token Optimization - Action Items

## ‚úÖ Completed

1. **Added Token Optimization Helper Methods**
   - `_extract_functions_for_review()` - Extract critical functions from code
   - `_needs_llm_review()` - Pre-screen simple files
   - `_select_model_for_task()` - Choose cheapest appropriate model
   - `_parse_pytest_output()` - Extract only test failures

2. **Updated Core Methods**
   - `_review_single_file()` - Function-level review with fallback
   - `_fast_qa_mode()` - Pre-screening for simple files
   - `_generate_fix_for_issue()` - Diff-based fix generation
   - `_run_generated_tests()` - Parsed output instead of full
   - `_llm_logic_review()` - Model selection

3. **Minimized Prompts**
   - Removed verbose instructions
   - Simplified system prompts
   - Focused on essential information

## üß™ Testing Checklist

### 1. Import Test
```bash
python -c "import agents.qa_agent; print('‚úÖ Import successful')"
```
**Status:** ‚úÖ Passed

### 2. Run Integration Tests
```bash
python -m pytest tests/test_integration_pm_dev_enhancements.py::TestTestGeneration -v
```
**Status:** ‚è≥ Pending

### 3. Test Simple File Pre-Screening
Create a test with `__init__.py` and verify it skips LLM review.
**Status:** ‚è≥ Pending

### 4. Test Function-Level Review
Create a test with a multi-function file and verify function extraction.
**Status:** ‚è≥ Pending

### 5. Test Diff-Based Fixes
Create a test with a line-specific issue and verify ¬±10 line extraction.
**Status:** ‚è≥ Pending

## üìä Monitoring Setup

### 1. Add Token Tracking
Monitor these metrics in production:
- Tokens per file review
- Tokens per fix generation
- Files skipped via pre-screening
- Model usage distribution

### 2. Cost Tracking
Track daily/weekly costs:
- Before optimization baseline
- After optimization actual
- Savings percentage

### 3. Performance Metrics
Monitor:
- QA completion time per task
- Fix success rate
- Issue detection accuracy

## üîß Configuration (Optional)

### Environment Variables
Add to `.env` if you want to customize thresholds:

```bash
# Function extraction limits
QA_MAX_FUNCTIONS_PER_FILE=3
QA_MIN_FUNCTION_LINES=5

# Pre-screening thresholds
QA_SIMPLE_FILE_MAX_CHARS=500
QA_SIMPLE_FILE_MAX_LINES=20
QA_SIMPLE_FILE_MIN_FUNCTIONS=2

# Diff-based fix context
QA_FIX_CONTEXT_LINES=10
QA_FIX_MAX_CHARS=1000

# Model selection thresholds
QA_FLASH_8B_MAX_CHARS=1000
QA_FLASH_2_MAX_CHARS=3000
```

**Note:** These are optional. Current hardcoded values work well.

## üìà Expected Results

### Week 1
- Monitor token usage logs
- Verify pre-screening is working (check for "‚è≠Ô∏è Skipping" logs)
- Verify function-level review (check for "üìä Reviewing X functions" logs)
- Compare costs with previous week

### Week 2
- Analyze token reduction percentage
- Verify fix success rate unchanged
- Check for any edge cases or issues
- Fine-tune thresholds if needed

### Week 3
- Calculate total savings
- Document any issues found
- Optimize further if needed

## üö® Potential Issues & Solutions

### Issue 1: Function Extraction Fails
**Symptom:** Logs show "Failed to extract functions" frequently
**Solution:** Fallback to full file review is automatic (already implemented)

### Issue 2: Pre-Screening Too Aggressive
**Symptom:** Missing issues in simple files
**Solution:** Adjust `_needs_llm_review()` criteria (reduce threshold from 3 to 2)

### Issue 3: Diff-Based Fixes Incomplete
**Symptom:** Fixes don't work because context is insufficient
**Solution:** Increase context from ¬±10 lines to ¬±15 lines

### Issue 4: Model Selection Too Cheap
**Symptom:** Lower quality reviews with cheaper models
**Solution:** Increase thresholds for model selection

## üéØ Success Criteria

### Must Have:
- ‚úÖ No breaking changes to existing functionality
- ‚úÖ Token usage reduced by at least 50%
- ‚úÖ Cost reduced by at least 50%
- ‚úÖ All existing tests pass

### Nice to Have:
- üéØ Token usage reduced by 65-80%
- üéØ Cost reduced by 75-90%
- üéØ Speed improvement of 30-50%
- üéØ No decrease in fix success rate

## üìù Next Steps

1. **Immediate (Today):**
   - ‚úÖ Code implementation complete
   - ‚è≥ Run integration tests
   - ‚è≥ Deploy to staging environment

2. **Short-term (This Week):**
   - Monitor token usage in staging
   - Collect baseline metrics
   - Compare with production baseline
   - Fix any issues found

3. **Medium-term (Next 2 Weeks):**
   - Deploy to production
   - Monitor for 1-2 weeks
   - Collect savings data
   - Document lessons learned

4. **Long-term (Next Month):**
   - Consider additional optimizations:
     - Batch file reviews
     - Response streaming with early stopping
     - Prompt caching API
     - Aggressive result caching

## üìû Support

If issues arise:
1. Check logs for error messages
2. Verify fallbacks are working
3. Review `QA_OPTIMIZATION_BEFORE_AFTER.md` for expected behavior
4. Adjust thresholds in helper methods if needed

---

**Status:** ‚úÖ Implementation Complete
**Next Action:** Run integration tests
**Owner:** Development Team
**Timeline:** Deploy to staging today, production next week
