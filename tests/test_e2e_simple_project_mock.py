"""
End-to-End test with MOCKED LLM calls for fast testing
This demonstrates Phase 2 architecture without waiting for actual LLM responses
"""
import pytest
import pytest_asyncio 
import asyncio
from unittest.mock import Mock, AsyncMock, patch, MagicMock
from pathlib import Path

from agents.pm_agent import PlannerAgent
from agents.dev_agent import DevAgent
from agents.qa_agent import QAAgent
from agents.ops_agent import OpsAgent
from parse.websocket_manager import WebSocketManager
from models.task import Task
from models.plan import Plan
from models.enums import TaskStatus


@pytest.fixture
def mock_websocket():
    """Create mock WebSocket."""
    ws = Mock()
    ws.send_text = AsyncMock()
    ws.send_json = AsyncMock()
    ws.client = Mock()
    ws.client.host = "127.0.0.1"
    ws.client.port = 12345
    return ws


@pytest_asyncio.fixture
async def agents(tmp_path):
    """Initialize all agents with mocked LLM."""
    ws_manager = WebSocketManager()
    
    pm_agent = PlannerAgent(ws_manager)
    pm_agent.generated_code_root = tmp_path / "generated"
    pm_agent.generated_code_root.mkdir(exist_ok=True)
    
    dev_agent = DevAgent(ws_manager)
    qa_agent = QAAgent(ws_manager)
    ops_agent = OpsAgent(ws_manager, pm_agent.generated_code_root)
    
    return {
        "pm": pm_agent,
        "dev": dev_agent,
        "qa": qa_agent,
        "ops": ops_agent,
        "ws_manager": ws_manager
    }


@pytest.mark.asyncio
async def test_simple_fastapi_workflow_mock(agents, mock_websocket, tmp_path):
    """Test complete workflow with MOCKED LLM responses - fast execution."""
    pm_agent = agents["pm"]
    dev_agent = agents["dev"]
    qa_agent = agents["qa"]
    ops_agent = agents["ops"]
    
    print("\n" + "="*60)
    print("ðŸ§ª E2E TEST: Simple FastAPI Application (MOCKED)")
    print("="*60 + "\n")
    
    # ============================================================
    # PHASE 1: PM Agent - Create Plan (MOCKED)
    # ============================================================
    print("ðŸ“‹ Phase 1: PM Agent Creating Plan...")
    
    # Create a mock plan directly instead of calling LLM
    mock_plan = Plan(
        id="test_plan_001",
        title="Simple FastAPI Application",
        description="Test plan for E2E workflow",
        tasks=[
            Task(
                id="task_001",
                title="Setup FastAPI Project",
                description="Create main.py with FastAPI app",
                priority=1,
                status=TaskStatus.PENDING,
                agent_type="dev_agent",
                dependencies=[],
                estimated_hours=1.0,
                complexity="low"
            ),
            Task(
                id="task_002",
                title="Implement Health Endpoint",
                description="Add /health endpoint",
                priority=2,
                status=TaskStatus.PENDING,
                agent_type="dev_agent",
                dependencies=["task_001"],
                estimated_hours=0.5,
                complexity="low"
            ),
            Task(
                id="task_003",
                title="Implement Hello Endpoint",
                description="Add /hello endpoint",
                priority=2,
                status=TaskStatus.PENDING,
                agent_type="dev_agent",
                dependencies=["task_001"],
                estimated_hours=0.5,
                complexity="low"
            )
        ]
    )
    
    pm_agent.current_plan = mock_plan
    
    print(f"  âœ“ Task 1: {mock_plan.tasks[0].title} (dev_agent)")
    print(f"  âœ“ Task 2: {mock_plan.tasks[1].title} (dev_agent)")
    print(f"  âœ“ Task 3: {mock_plan.tasks[2].title} (dev_agent)")
    print(f"\nâœ… PM Agent created {len(mock_plan.tasks)} tasks\n")
    
    # ============================================================
    # PHASE 2: Dev Agent - Execute Development Tasks (MOCKED)
    # ============================================================
    print("ðŸ’» Phase 2: Dev Agent Executing Tasks...")
    
    dev_tasks = [t for t in mock_plan.tasks if t.agent_type == "dev_agent"]
    dev_results = []
    
    for idx, dev_task in enumerate(dev_tasks, 1):
        print(f"\n  Task {idx}/{len(dev_tasks)}: {dev_task.title}")
        
        # Mock the LLM response for code generation
        mock_code = f"""
# Generated by Dev Agent - Task: {dev_task.title}
from fastapi import FastAPI

app = FastAPI()

@app.get("/health")
def health_check():
    return {{"status": "healthy"}}

@app.get("/hello")
def hello():
    return {{"message": "Hello World"}}
"""
        
        # Create a mock output file
        output_dir = pm_agent.generated_code_root / f"task_{dev_task.id}"
        output_dir.mkdir(exist_ok=True)
        output_file = output_dir / "main.py"
        output_file.write_text(mock_code)
        
        # Update task status
        dev_task.status = TaskStatus.COMPLETED
        dev_task.result = f"Code generated successfully for {dev_task.title}"
        dev_task.metadata = {
            "file_path": str(output_file),
            "lines_of_code": len(mock_code.split('\n'))
        }
        
        dev_results.append(dev_task)
        
        print(f"    âœ“ Generated: {output_file.name}")
        print(f"    âœ“ Status: {dev_task.status.value}")
    
    print(f"\nâœ… Dev Agent completed {len(dev_results)} tasks\n")
    
    # ============================================================
    # PHASE 3: QA Agent - Test Generated Code (MOCKED)
    # ============================================================
    print("ðŸ§ª Phase 3: QA Agent Testing Code...")
    
    qa_results = []
    
    for idx, dev_task in enumerate(dev_results, 1):
        print(f"\n  Testing {idx}/{len(dev_results)}: {dev_task.title}")
        
        # Mock QA test results
        total_tests = 5
        passed_tests = 5
        
        dev_task.status = TaskStatus.COMPLETED
        dev_task.metadata["total_tests"] = total_tests
        dev_task.metadata["passed_tests"] = passed_tests
        dev_task.metadata["failed_tests"] = 0
        
        qa_results.append(dev_task)
        
        print(f"    âœ“ Tests: {passed_tests}/{total_tests} passed")
        print(f"    âœ“ Status: {dev_task.status.value}")
    
    print(f"\nâœ… QA Agent tested {len(qa_results)} components\n")
    
    # ============================================================
    # PHASE 4: Ops Agent - Deploy to Production (MOCKED)
    # ============================================================
    print("ðŸš€ Phase 4: Ops Agent Deploying...")
    
    deployment_task = Task(
        id="deploy_simple_fastapi",
        title="Deploy FastAPI Application",
        description="Production deployment of simple FastAPI app",
        priority=10,
        status=TaskStatus.PENDING,
        agent_type="ops_agent",
        metadata={
            "project_type": "fastapi",
            "dev_tasks_completed": len(dev_results),
            "qa_tasks_passed": len([r for r in qa_results if r.status == TaskStatus.COMPLETED])
        }
    )
    
    # Mock deployment result
    deployment_task.status = TaskStatus.COMPLETED
    deployment_task.result = "Deployment completed successfully"
    deployment_task.metadata["github_url"] = "https://github.com/test/simple-fastapi"
    deployment_task.metadata["deployment_urls"] = [
        {"platform": "Render", "url": "https://simple-fastapi.onrender.com"},
        {"platform": "Railway", "url": "https://simple-fastapi.railway.app"}
    ]
    
    print(f"\n  âœ“ Deployment Status: {deployment_task.status.value}")
    print(f"  âœ“ GitHub: {deployment_task.metadata['github_url']}")
    
    for deployment in deployment_task.metadata['deployment_urls']:
        print(f"  âœ“ Deployed: {deployment['platform']} â†’ {deployment['url']}")
    
    print(f"\nâœ… Ops Agent deployment {deployment_task.status.value}")
    
    # ============================================================
    # SUMMARY
    # ============================================================
    print("\n" + "="*60)
    print("ðŸ“Š E2E TEST SUMMARY (MOCKED)")
    print("="*60)
    print(f"âœ… PM Agent:   {len(mock_plan.tasks)} tasks created")
    print(f"âœ… Dev Agent:  {len(dev_results)}/{len(dev_tasks)} tasks completed")
    print(f"âœ… QA Agent:   {len(qa_results)}/{len(dev_results)} components tested")
    print(f"âœ… Ops Agent:  Deployment {deployment_task.status.value}")
    print("="*60)
    print("ðŸŽ‰ END-TO-END WORKFLOW COMPLETED SUCCESSFULLY!")
    print("="*60 + "\n")
    
    # Assertions
    assert len(mock_plan.tasks) == 3
    assert len(dev_results) == 3
    assert all(t.status == TaskStatus.COMPLETED for t in dev_results)
    assert len(qa_results) == 3
    assert deployment_task.status == TaskStatus.COMPLETED
    
    print("âœ… All assertions passed!")


@pytest.mark.asyncio
async def test_parallel_execution_mock(agents, mock_websocket, tmp_path):
    """Test parallel execution demonstration with mocked tasks."""
    pm_agent = agents["pm"]
    
    print("\n" + "="*60)
    print("ðŸ§ª E2E TEST: Parallel Execution (MOCKED)")
    print("="*60 + "\n")
    
    # Create 4 independent tasks
    tasks = [
        Task(
            id=f"task_{i:03d}",
            title=f"Service {i}",
            description=f"Independent service {i}",
            priority=1,
            status=TaskStatus.PENDING,
            agent_type="dev_agent",
            dependencies=[],
            estimated_hours=1.0,
            complexity="medium"
        )
        for i in range(1, 5)
    ]
    
    print(f"ðŸ“‹ Created {len(tasks)} independent tasks\n")
    
    # Simulate sequential execution
    import time
    print("â±ï¸  Sequential Execution:")
    start = time.time()
    for task in tasks:
        await asyncio.sleep(0.1)  # Simulate work
        task.status = TaskStatus.COMPLETED
    sequential_time = time.time() - start
    print(f"  âœ“ Completed in {sequential_time:.2f}s\n")
    
    # Simulate parallel execution
    print("âš¡ Parallel Execution:")
    start = time.time()
    
    async def process_task(task):
        await asyncio.sleep(0.1)  # Simulate work
        task.status = TaskStatus.COMPLETED
        return task
    
    await asyncio.gather(*[process_task(t) for t in tasks])
    parallel_time = time.time() - start
    print(f"  âœ“ Completed in {parallel_time:.2f}s\n")
    
    # Calculate speedup
    speedup = sequential_time / parallel_time if parallel_time > 0 else 0
    
    print("="*60)
    print("ðŸ“Š PARALLEL EXECUTION ANALYSIS")
    print("="*60)
    print(f"Tasks:           {len(tasks)}")
    print(f"Sequential:      {sequential_time:.2f}s")
    print(f"Parallel:        {parallel_time:.2f}s")
    print(f"Speedup:         {speedup:.2f}x")
    print(f"Time Saved:      {(sequential_time - parallel_time):.2f}s")
    print("="*60)
    
    assert parallel_time < sequential_time, "Parallel should be faster"
    assert speedup > 1.5, f"Should achieve >1.5x speedup, got {speedup:.2f}x"
    
    print("ðŸŽ‰ PARALLEL EXECUTION TEST PASSED!\n")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])